{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5_ja-qiita-title-generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-LrpO7Zm4s-"
      },
      "source": [
        "# 深層学習モデルを用いたQiita記事のタイトル自動生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzRmZ5PynZ1I"
      },
      "source": [
        "## 依存ライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JQI6sIAAR2T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34baf850-0464-482f-e515-659057512b28"
      },
      "source": [
        "!pip install -qU torch==1.7.1 torchtext==0.8.0 torchvision==0.8.2\n",
        "!pip install -q transformers==4.2.2 sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 776.8MB 23kB/s \n",
            "\u001b[K     |████████████████████████████████| 7.0MB 33.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 12.8MB 50.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8MB 13.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 50.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 890kB 54.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9MB 54.5MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gj4zzBDHm35D"
      },
      "source": [
        "## 学習済みモデルのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykB9VLY0geeu",
        "outputId": "a330b006-43e6-4db1-86e0-ad4e04b8ebeb"
      },
      "source": [
        "!mkdir -p model\n",
        "!wget -O qiita_title_generation_model.tar \"https://www.floydhub.com/api/v1/resources/fixnot7HrfCYhLWwgEeskL?content=true&download=true\"\n",
        "!tar xvf qiita_title_generation_model.tar -C model"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-15 13:03:08--  https://www.floydhub.com/api/v1/resources/fixnot7HrfCYhLWwgEeskL?content=true&download=true\n",
            "Resolving www.floydhub.com (www.floydhub.com)... 104.26.0.30, 104.26.1.30, 172.67.72.144, ...\n",
            "Connecting to www.floydhub.com (www.floydhub.com)|104.26.0.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/tar]\n",
            "Saving to: ‘qiita_title_generation_model.tar’\n",
            "\n",
            "qiita_title_generat     [     <=>            ] 851.20M  15.9MB/s    in 55s     \n",
            "\n",
            "2021-03-15 13:04:04 (15.6 MB/s) - ‘qiita_title_generation_model.tar’ saved [892547584]\n",
            "\n",
            "./\n",
            "./config.json\n",
            "./pytorch_model.bin\n",
            "./special_tokens_map.json\n",
            "./spiece.model\n",
            "./tokenizer_config.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFB22ZGjnMAI"
      },
      "source": [
        "## 学習済みモデルの読み込み\n",
        "\n",
        "この深層学習モデルは、日本語コーパスを用いて事前学習したT5モデルを今回のタイトル生成タスク向けに転移学習したものです。  \n",
        "\n",
        "T5とは: https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part7.html\n",
        "\n",
        "今回のモデルの入出力は次の形式になっています。\n",
        "\n",
        "- **入力**: \"body: {body}\"をトークナイズしたもの\n",
        "- **出力**: \"title: {title}\"をトークナイズしたもの\n",
        "\n",
        "ここで、{body}はMarkdown形式の記事本文を前処理した文字列、{title}は生成されたタイトルです。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHvq7JNCQtvD"
      },
      "source": [
        "import torch\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\r\n",
        "from transformers import AdamW,get_linear_schedule_with_warmup\r\n",
        "\r\n",
        "# 学習済みモデルが格納されたディレクトリ\r\n",
        "model_dir_name = \"/content/model/\"\r\n",
        "\r\n",
        "# トークナイザー（SentencePiece）\r\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_dir_name, is_fast=True)\r\n",
        "\r\n",
        "# 学習済みモデル\r\n",
        "trained_model = T5ForConditionalGeneration.from_pretrained(model_dir_name)\r\n",
        "\r\n",
        "# GPUの利用有無\r\n",
        "USE_GPU = torch.cuda.is_available()\r\n",
        "if USE_GPU:\r\n",
        "  trained_model.cuda()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haPFE6-F90-f"
      },
      "source": [
        "## 前処理の定義"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQYfJ3ZipSuM"
      },
      "source": [
        "### 文字列の正規化\n",
        "\n",
        "表記揺れを減らします。今回は[neologdの正規化処理](https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja)を一部改変したものを利用します。\n",
        "処理の詳細はリンク先を参照してください。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAl6qQhJFJA3"
      },
      "source": [
        "# https://github.com/neologd/mecab-ipadic-neologd/wiki/Regexp.ja から引用・一部改変\n",
        "from __future__ import unicode_literals\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "def unicode_normalize(cls, s):\n",
        "    pt = re.compile('([{}]+)'.format(cls))\n",
        "\n",
        "    def norm(c):\n",
        "        return unicodedata.normalize('NFKC', c) if pt.match(c) else c\n",
        "\n",
        "    s = ''.join(norm(x) for x in re.split(pt, s))\n",
        "    s = re.sub('－', '-', s)\n",
        "    return s\n",
        "\n",
        "def remove_extra_spaces(s):\n",
        "    s = re.sub('[ 　]+', ' ', s)\n",
        "    blocks = ''.join(('\\u4E00-\\u9FFF',  # CJK UNIFIED IDEOGRAPHS\n",
        "                      '\\u3040-\\u309F',  # HIRAGANA\n",
        "                      '\\u30A0-\\u30FF',  # KATAKANA\n",
        "                      '\\u3000-\\u303F',  # CJK SYMBOLS AND PUNCTUATION\n",
        "                      '\\uFF00-\\uFFEF'   # HALFWIDTH AND FULLWIDTH FORMS\n",
        "                      ))\n",
        "    basic_latin = '\\u0000-\\u007F'\n",
        "\n",
        "    def remove_space_between(cls1, cls2, s):\n",
        "        p = re.compile('([{}]) ([{}])'.format(cls1, cls2))\n",
        "        while p.search(s):\n",
        "            s = p.sub(r'\\1\\2', s)\n",
        "        return s\n",
        "\n",
        "    s = remove_space_between(blocks, blocks, s)\n",
        "    s = remove_space_between(blocks, basic_latin, s)\n",
        "    s = remove_space_between(basic_latin, blocks, s)\n",
        "    return s\n",
        "\n",
        "def normalize_neologd(s):\n",
        "    s = s.strip()\n",
        "    s = unicode_normalize('０-９Ａ-Ｚａ-ｚ｡-ﾟ', s)\n",
        "\n",
        "    def maketrans(f, t):\n",
        "        return {ord(x): ord(y) for x, y in zip(f, t)}\n",
        "\n",
        "    s = re.sub('[˗֊‐‑‒–⁃⁻₋−]+', '-', s)  # normalize hyphens\n",
        "    s = re.sub('[﹣－ｰ—―─━ー]+', 'ー', s)  # normalize choonpus\n",
        "    s = re.sub('[~∼∾〜〰～]+', '〜', s)  # normalize tildes (modified by Isao Sonobe)\n",
        "    s = s.translate(\n",
        "        maketrans('!\"#$%&\\'()*+,-./:;<=>?@[¥]^_`{|}~｡､･｢｣',\n",
        "              '！”＃＄％＆’（）＊＋，－．／：；＜＝＞？＠［￥］＾＿｀｛｜｝〜。、・「」'))\n",
        "\n",
        "    s = remove_extra_spaces(s)\n",
        "    s = unicode_normalize('！”＃＄％＆’（）＊＋，－．／：；＜＞？＠［￥］＾＿｀｛｜｝〜', s)  # keep ＝,・,「,」\n",
        "    s = re.sub('[’]', '\\'', s)\n",
        "    s = re.sub('[”]', '\"', s)\n",
        "    return s"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk82nUHjqkMh"
      },
      "source": [
        "### Markdownのクリーニング\n",
        "\n",
        "タイトルを考える上で関係のなさそうな文章を削る処理を行います。  \n",
        "以下のノイズとなるデータを削除し、タブや改行を空白文字にしたり、文字を小文字に揃える等の処理を行います。\n",
        "\n",
        "- ソースコード\n",
        "- URLやリンク\n",
        "- 画像\n",
        "\n",
        "現状、img以外のHTML要素を残していますが、タイトルに関係なさそうな要素を削ると精度が上がるかもしれません。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYkCAVDIdOoO"
      },
      "source": [
        "import re\n",
        "\n",
        "CODE_PATTERN = re.compile(r\"```.*?```\", re.MULTILINE | re.DOTALL)\n",
        "LINK_PATTERN = re.compile(r\"!?\\[([^\\]\\)]+)\\]\\([^\\)]+\\)\")\n",
        "IMG_PATTERN = re.compile(r\"<img[^>]*>\")\n",
        "URL_PATTERN = re.compile(r\"(http|ftp)s?://[^\\s]+\")\n",
        "NEWLINES_PATTERN = re.compile(r\"(\\s*\\n\\s*)+\")\n",
        "\n",
        "def clean_markdown(markdown_text):\n",
        "    markdown_text = CODE_PATTERN.sub(r\"\", markdown_text)\n",
        "    markdown_text = LINK_PATTERN.sub(r\"\\1\", markdown_text)\n",
        "    markdown_text = IMG_PATTERN.sub(r\"\", markdown_text)\n",
        "    markdown_text = URL_PATTERN.sub(r\"\", markdown_text)\n",
        "    markdown_text = NEWLINES_PATTERN.sub(r\"\\n\", markdown_text)\n",
        "    markdown_text = markdown_text.replace(\"`\", \"\")\n",
        "    return markdown_text\n",
        "\n",
        "def normalize_text(markdown_text):\n",
        "    markdown_text = clean_markdown(markdown_text)\n",
        "    markdown_text = markdown_text.replace(\"\\t\", \" \")\n",
        "    markdown_text = normalize_neologd(markdown_text).lower()\n",
        "    markdown_text = markdown_text.replace(\"\\n\", \" \")\n",
        "    return markdown_text\n",
        "\n",
        "def preprocess_qiita_body(markdown_text):\n",
        "    return \"body: \" + normalize_text(markdown_text)[:4000]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO6ghZEouTpu"
      },
      "source": [
        "## 後処理の定義\n",
        "\n",
        "モデルの出力は\"title: {title}\"という形式ですので、後処理として余計な\"title: \"を削除するようにします。\n",
        "\n",
        "これで必要な前処理、後処理を定義できました。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQlUn1MEgnv7"
      },
      "source": [
        "def postprocess_title(title):\r\n",
        "  return re.sub(r\"^title: \", \"\", title)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R08RgDSJuF0Z"
      },
      "source": [
        "## タイトル生成の対象となる記事本文の定義\n",
        "\n",
        "今回は下記のような記事に対するタイトル生成を試してみます。  \n",
        "中身を色々変えて試してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOACupDad9kx"
      },
      "source": [
        "qiita_body = \"\"\"\n",
        "AIの進歩はすごいですね。\n",
        "\n",
        "今回は深層学習を用いて、記事（Qiita）のタイトルを自動生成してくれるAIさんを試作してみました。\n",
        "この実験は自然言語処理について新人さんに教えるためのハンズオンネタを探索する一環で行ったものになります。\n",
        "\n",
        "作ったAIは、Qiitaの記事本文（少し前処理したテキスト）を与えると、適したタイトル文字列を作文して返してくれるというものです。\n",
        "\n",
        "なお、学習データは（2019年頃に）Qiitaの殿堂を入り口にして、評価の高い記事（いいねが50個以上）をスクレイピングしたものを使いました。\n",
        "つまりヒットした記事のタイトルの付け方を学んだAIであるといえます。\n",
        "\n",
        "* もう少し詳細:\n",
        "  * 学習データの例:\n",
        "    * 入力: \"body: hiveqlではスピードに難を感じていたため、私もprestoを使い始めました。 mysqlやhiveで使っていたクエリ...\"\n",
        "    * 出力: \"title: hadoop利用者ならきっと知ってる、hive/prestoクエリ関数の挙動の違い\"\n",
        "  * 学習方法: 独自に作った日本語T5の事前学習モデルをこの学習データを用いて転移学習\n",
        "\n",
        "以下、結果（抜粋）です。generatedが生成されたもの、actualが人が付けたタイトルです。\n",
        "\"\"\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-L-XRHHuVul"
      },
      "source": [
        "この文章に対して前処理をすると次のようになります。これをトークナイズしたものがモデルの入力になります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJZTmtnKeo6v",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "318ceaaf-2eb7-41ce-eb7e-6f18802a5c39"
      },
      "source": [
        "preprocess_qiita_body(qiita_body)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'body: aiの進歩はすごいですね。 今回は深層学習を用いて、記事(qiita)のタイトルを自動生成してくれるaiさんを試作してみました。 この実験は自然言語処理について新人さんに教えるためのハンズオンネタを探索する一環で行ったものになります。 作ったaiは、qiitaの記事本文(少し前処理したテキスト)を与えると、適したタイトル文字列を作文して返してくれるというものです。 なお、学習データは(2019年頃に)qiitaの殿堂を入り口にして、評価の高い記事(いいねが50個以上)をスクレイピングしたものを使いました。 つまりヒットした記事のタイトルの付け方を学んだaiであるといえます。 *もう少し詳細: *学習データの例: *入力:\\xa0\"body:\\xa0hiveqlではスピードに難を感じていたため、私もprestoを使い始めました。\\xa0mysqlやhiveで使っていたクエリ...\" *出力:\\xa0\"title:\\xa0hadoop利用者ならきっと知ってる、hive/prestoクエリ関数の挙動の違い\" *学習方法:\\xa0独自に作った日本語t5の事前学習モデルをこの学習データを用いて転移学習 以下、結果(抜粋)です。generatedが生成されたもの、actualが人が付けたタイトルです。'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrnOaob2HOeW"
      },
      "source": [
        "## タイトルの自動生成を実行\n",
        "\n",
        "記事に合うタイトルを10個、自動生成してみます。\n",
        "\n",
        "以下のコードではタイトルの多様性を生むために色々generateメソッドのパラメータを設定しています。パラメータの詳細は下記リンク先を参照してください。\n",
        "\n",
        "https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.generate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xS-GsxCetcK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f29a3ba-d3f1-4417-8df0-d2af14f03675"
      },
      "source": [
        "MAX_SOURCE_LENGTH = 512  # 入力される記事本文の最大トークン数\r\n",
        "MAX_TARGET_LENGTH = 64   # 生成されるタイトルの最大トークン数\r\n",
        "\r\n",
        "# 推論モード設定\r\n",
        "trained_model.eval()\r\n",
        "\r\n",
        "# 前処理とトークナイズを行う\r\n",
        "inputs = [preprocess_qiita_body(qiita_body)]\r\n",
        "batch = tokenizer.batch_encode_plus(\r\n",
        "    inputs, max_length=MAX_SOURCE_LENGTH, truncation=True, \r\n",
        "    padding=\"longest\", return_tensors=\"pt\")\r\n",
        "\r\n",
        "input_ids = batch['input_ids']\r\n",
        "input_mask = batch['attention_mask']\r\n",
        "if USE_GPU:\r\n",
        "  input_ids = input_ids.cuda()\r\n",
        "  input_mask = input_mask.cuda()\r\n",
        "\r\n",
        "# 生成処理を行う\r\n",
        "outputs = trained_model.generate(\r\n",
        "    input_ids=input_ids, attention_mask=input_mask, \r\n",
        "    max_length=MAX_TARGET_LENGTH,\r\n",
        "    return_dict_in_generate=True, output_scores=True,\r\n",
        "    temperature=1.0,            # 生成にランダム性を入れる温度パラメータ\r\n",
        "    num_beams=10,               # ビームサーチの探索幅\r\n",
        "    diversity_penalty=1.0,      # 生成結果の多様性を生み出すためのペナルティ\r\n",
        "    num_beam_groups=10,         # ビームサーチのグループ数\r\n",
        "    num_return_sequences=10,    # 生成する文の数\r\n",
        "    repetition_penalty=1.5,     # 同じ文の繰り返し（モード崩壊）へのペナルティ\r\n",
        ")\r\n",
        "\r\n",
        "# 生成されたトークン列を文字列に変換する\r\n",
        "generated_titles = [tokenizer.decode(ids, skip_special_tokens=True, \r\n",
        "                                     clean_up_tokenization_spaces=False) \r\n",
        "                    for ids in outputs.sequences]\r\n",
        "\r\n",
        "# 生成されたタイトルを表示する\r\n",
        "for i, title in enumerate(generated_titles):\r\n",
        "  print(f\"{i+1:2}. {postprocess_title(title)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 1. 深層学習でqiitaのタイトルを自動生成してくれるaiを試作した\n",
            " 2. aiの進化はすごい。記事タイトル自動生成のaiを試作した\n",
            " 3. ディープラーニングで記事のタイトルを自動生成してくれるaiを試作した\n",
            " 4. ディープラーニングで記事のタイトルを自動生成してくれるaiを試作した。\n",
            " 5. aiがすごい勢いで記事のタイトルを自動生成するaiを試作した\n",
            " 6. 【人工知能】深層学習で「記事タイトルを自動生成」する\n",
            " 7. deep learningでqiitaのタイトルを自動生成するaiを作ってみた\n",
            " 8. deep learningでqiitaのタイトルを自動生成するaiを作ってみた。\n",
            " 9. 「記事のタイトルを自動生成してくれるai」を作ってみた。(結果)\n",
            "10. 「記事のタイトルを自動生成してくれるai」を作ってみた\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TU3L01WgKmE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}